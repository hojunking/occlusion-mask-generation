{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "def remove_duplicate_images_phash(image_folder, output_folder, hash_threshold=5):\n",
    "    \"\"\"\n",
    "    Perceptual Hashing을 사용하여 중복 이미지를 제거\n",
    "    Args:\n",
    "        image_folder (str): 원본 이미지 폴더 경로\n",
    "        output_folder (str): 중복 제거 후 저장할 폴더 경로\n",
    "        hash_threshold (int): 해시값 간 차이 허용 범위 (작을수록 엄격)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    hash_dict = {}\n",
    "    unique_images = []\n",
    "\n",
    "    for filename in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        if not filename.lower().endswith(('png', 'jpg', 'jpeg', 'bmp')):\n",
    "            continue\n",
    "\n",
    "        # Perceptual Hash 계산\n",
    "        img = Image.open(image_path)\n",
    "        img_hash = imagehash.phash(img)\n",
    "\n",
    "        # 중복 체크\n",
    "        is_duplicate = False\n",
    "        for existing_hash in hash_dict:\n",
    "            if abs(img_hash - existing_hash) <= hash_threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            hash_dict[img_hash] = filename\n",
    "            unique_images.append(filename)\n",
    "            img.save(os.path.join(output_folder, filename))\n",
    "\n",
    "    print(f\"Unique images saved: {len(unique_images)}\")\n",
    "    return unique_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import shutil\n",
    "\n",
    "def remove_duplicate_images_with_count(image_folder, json_folder, hash_difference=5):\n",
    "    \"\"\"\n",
    "    중복 이미지를 제거하고, 제거 전후 이미지 개수를 출력합니다.\n",
    "    \n",
    "    Args:\n",
    "        image_folder (str): 이미지가 저장된 폴더 경로.\n",
    "        json_folder (str): 이미지와 매칭된 JSON 파일들이 있는 폴더 경로.\n",
    "        hash_difference (int): 허용하는 해시 차이(민감도).\n",
    "    \"\"\"\n",
    "    # 이미지 목록 가져오기\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Initial image count: {len(image_files)}\")  # 초기 이미지 개수 출력\n",
    "\n",
    "    # 해시값 저장을 위한 딕셔너리\n",
    "    hash_dict = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        # 이미지 해시 계산\n",
    "        image = Image.open(image_path)\n",
    "        image_hash = imagehash.average_hash(image)\n",
    "        image.close()\n",
    "\n",
    "        # 중복 여부 확인\n",
    "        found_duplicate = False\n",
    "        for stored_hash, stored_file in hash_dict.items():\n",
    "            if abs(image_hash - stored_hash) <= hash_difference:\n",
    "                duplicates.append(image_file)\n",
    "                found_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if not found_duplicate:\n",
    "            hash_dict[image_hash] = image_file  # 해시 딕셔너리에 추가\n",
    "\n",
    "    # 중복 이미지 제거\n",
    "    for duplicate_file in duplicates:\n",
    "        # 이미지 삭제\n",
    "        duplicate_image_path = os.path.join(image_folder, duplicate_file)\n",
    "        os.remove(duplicate_image_path)\n",
    "\n",
    "        # JSON 파일 삭제\n",
    "        duplicate_json_path = os.path.join(json_folder, os.path.splitext(duplicate_file)[0] + \".json\")\n",
    "        if os.path.exists(duplicate_json_path):\n",
    "            os.remove(duplicate_json_path)\n",
    "\n",
    "    # 최종 이미지 개수 출력\n",
    "    final_image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Final image count: {len(final_image_files)}\")  # 중복 제거 후 이미지 개수 출력\n",
    "    print(f\"Removed {len(duplicates)} duplicate images.\")  # 제거된 중복 이미지 개수 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial image count: 601\n",
      "Final image count: 450\n",
      "Removed 151 duplicate images.\n"
     ]
    }
   ],
   "source": [
    "# 경로 설정\n",
    "image_folder = \"/home/knuvi/Desktop/song/cucumber-image/data/whole_oi/images\"\n",
    "json_folder = \"/home/knuvi/Desktop/song/cucumber-image/data/whole_oi/labels\"\n",
    "\n",
    "# 민감도 순차적으로 조정하며 실행\n",
    "remove_duplicate_images_with_count(image_folder, json_folder, hash_difference=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_song",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
